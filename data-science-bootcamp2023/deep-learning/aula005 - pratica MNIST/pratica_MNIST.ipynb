{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JKgTdVTzt6N1",
        "outputId": "8043d1f2-7e7a-472e-c536-68738f67adb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlano de ação do MNIST\\n1. Prepare os dados e pré-processe. Crie datasets de treinamento, validação e teste.\\n2. Defina o modelo e escolha as funções de ativação.\\n3. Defina os otimizadores avançados adequados e a função de perda.\\n4. Faça-o aprender.\\n5. Teste a precisão do modelo usando o dataset de teste.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "Plano de ação do MNIST\n",
        "1. Prepare os dados e pré-processe. Crie datasets de treinamento, validação e teste.\n",
        "2. Defina o modelo e escolha as funções de ativação.\n",
        "3. Defina os otimizadores avançados adequados e a função de perda.\n",
        "4. Faça-o aprender.\n",
        "5. Teste a precisão do modelo usando o dataset de teste.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "YVXM6PyCwwFV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datset\n",
        "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "4AgPWYW_xHGX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datset Preprocessing\n",
        "mnist_train, mnist_test = mnist_dataset[\"train\"], mnist_dataset[\"test\"]\n",
        "\n",
        "num_test_samples = mnist_info.splits[\"test\"].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.\n",
        "  return image, label\n",
        "\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "metadata": {
        "id": "wqgzEE8rylHB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Implementation\n",
        "# Model Hyperparameters: inputLayer = 784; hiddenLayers: 2 X 100; outputLayer: 10\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
        "                            tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
        "                            ])"
      ],
      "metadata": {
        "id": "jkB39iIlBbh9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "bX19cETfN9iG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "NUM_EPOCHS = 5\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFjuxyCdPHZf",
        "outputId": "07498a49-1dfc-475d-a578-55895d1c58db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 16s - loss: 0.3305 - accuracy: 0.9057 - val_loss: 0.1693 - val_accuracy: 0.9502 - 16s/epoch - 29ms/step\n",
            "Epoch 2/5\n",
            "540/540 - 7s - loss: 0.1339 - accuracy: 0.9607 - val_loss: 0.1061 - val_accuracy: 0.9700 - 7s/epoch - 13ms/step\n",
            "Epoch 3/5\n",
            "540/540 - 4s - loss: 0.0941 - accuracy: 0.9729 - val_loss: 0.1016 - val_accuracy: 0.9702 - 4s/epoch - 7ms/step\n",
            "Epoch 4/5\n",
            "540/540 - 4s - loss: 0.0723 - accuracy: 0.9771 - val_loss: 0.0775 - val_accuracy: 0.9778 - 4s/epoch - 7ms/step\n",
            "Epoch 5/5\n",
            "540/540 - 4s - loss: 0.0561 - accuracy: 0.9827 - val_loss: 0.0701 - val_accuracy: 0.9777 - 4s/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a3494633070>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Test (model deployment)\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print(f\"Test loss: {test_loss}. Test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6PIfEueUMWh",
        "outputId": "3b94b951-095a-4f05-a2e8-fe5538d329cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 809ms/step - loss: 0.0836 - accuracy: 0.9753\n",
            "Test loss: 0.08355484902858734. Test accuracy: 0.9753000140190125\n"
          ]
        }
      ]
    }
  ]
}